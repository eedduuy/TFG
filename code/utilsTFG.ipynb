{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Nkd_qf_RwVEMR9JkiOUi2qFEWjtoMtf-",
      "authorship_tag": "ABX9TyP6K1g/2eeUVtzqTNeKTAg7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eedduu/TFG/blob/main/code/utilsTFG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "import fastai\n",
        "from fastai.vision.all import *\n",
        "from fastai.metrics import *\n",
        "# Set seed for fastai\n",
        "\n",
        "\n",
        "# Set seed for torch\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# Set seed for numpy\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "O4aajiqAEBKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFS6dMhz6Za-"
      },
      "outputs": [],
      "source": [
        "#Resnet32 model\n",
        "\n",
        "class BottleneckBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, bottleneck_ratio=4):\n",
        "        super(BottleneckBlock, self).__init__()\n",
        "        bottleneck_channels = out_channels // bottleneck_ratio\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, bottleneck_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(bottleneck_channels)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(bottleneck_channels, bottleneck_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(bottleneck_channels)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(bottleneck_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.downsample = None\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNet57(nn.Module):\n",
        "    def __init__(self, num_classes=1000, in_channels=1):\n",
        "        super(ResNet57, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.conv1 = nn.Conv2d(self.in_channels, 64, kernel_size=7, stride=1, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(64)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(64, 3, stride=1, input_channels=64)\n",
        "        self.layer2 = self._make_layer(128, 4, stride=2)\n",
        "        self.layer3 = self._make_layer(256, 4, stride=2)\n",
        "        self.layer5 = self._make_layer(512, 3, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(512, num_classes)\n",
        "        #self.bn2 = nn.BatchNorm1d(256)\n",
        "        #self.dropout2 = nn.Dropout(0.5)\n",
        "        #self.fc2 = nn.Linear(256, num_classes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def _make_layer(self, out_channels, blocks, stride, input_channels=None):\n",
        "        layers = []\n",
        "\n",
        "        # Set input channels for the first block of the first layer\n",
        "        if input_channels:\n",
        "            self.in_channels = input_channels\n",
        "\n",
        "        layers.append(BottleneckBlock(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(BottleneckBlock(self.in_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer5(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.bn1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc1(x)\n",
        "       # x = self.relu(x)\n",
        "       # x = self.bn2(x)\n",
        "       # x = self.dropout2(x)\n",
        "       # x = self.fc2(x)\n",
        "\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class ResNet15(nn.Module):\n",
        "    def __init__(self, num_classes=1000, in_channels=1):\n",
        "        super(ResNet15, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.conv1 = nn.Conv2d(self.in_channels, 64, kernel_size=7, stride=1, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(64)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(64, 1, stride=1, input_channels=64)\n",
        "        self.layer2 = self._make_layer(128, 1, stride=2)\n",
        "        self.layer3 = self._make_layer(256, 1, stride=2)\n",
        "        self.layer5 = self._make_layer(512, 1, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(512, num_classes)\n",
        "        #self.bn2 = nn.BatchNorm1d(256)\n",
        "        #self.dropout2 = nn.Dropout(0.5)\n",
        "        #self.fc2 = nn.Linear(256, num_classes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def _make_layer(self, out_channels, blocks, stride, input_channels=None):\n",
        "        layers = []\n",
        "\n",
        "        # Set input channels for the first block of the first layer\n",
        "        if input_channels:\n",
        "            self.in_channels = input_channels\n",
        "\n",
        "        layers.append(BottleneckBlock(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(BottleneckBlock(self.in_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer5(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.bn1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc1(x)\n",
        "       # x = self.relu(x)\n",
        "       # x = self.bn2(x)\n",
        "       # x = self.dropout2(x)\n",
        "       # x = self.fc2(x)\n",
        "\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LeNet 5 model\n",
        "\n",
        "class LeNet5(nn.Module):\n",
        "  def __init__(self, in_channels=1, num_classes=10):\n",
        "    super(LeNet5, self).__init__()\n",
        "\n",
        "    self.in_channels = in_channels\n",
        "    self.conv1 = nn.Conv2d(in_channels, 6, kernel_size=5, stride=1)\n",
        "    self.bn1 =  nn.BatchNorm2d(6)\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)\n",
        "    self.bn2 =  nn.BatchNorm2d(16)\n",
        "\n",
        "\n",
        "    self.fc1 = nn.Linear(400, 120)\n",
        "    self.bn3 =  nn.BatchNorm1d(120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.bn4 =  nn.BatchNorm1d(84)\n",
        "    self.fc3 = nn.Linear(84, num_classes)\n",
        "\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.bn3(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.bn4(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "rbXRFZd6U-PG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, balanced_accuracy_score, r2_score\n",
        "\n",
        "# Define a function to compute the F1 score\n",
        "def f1_score_func(preds, targets):\n",
        "  preds = preds.argmax(dim=-1)  # Convert predictions to class labels\n",
        "  return f1_score(targets.cpu().numpy(), preds.cpu().numpy(), average='weighted')  # Calculate F1 score\n",
        "\n",
        "\n",
        "def balanced_accuracy_func(preds, targs):\n",
        "    preds_proba = preds.softmax(dim=-1).cpu().numpy()  # Convert logits to probabilities\n",
        "    targs = targs.squeeze().cpu().numpy()  # Convert targets to numpy array\n",
        "\n",
        "    # Convert probabilities to class predictions\n",
        "    preds_classes = preds_proba.argmax(axis=-1)\n",
        "\n",
        "    # Compute balanced accuracy score\n",
        "    return balanced_accuracy_score(targs, preds_classes)\n",
        "\n",
        "\n",
        "# Define the Adjusted RÂ² function\n",
        "\n",
        "def adjusted_r2_score(y_true, y_pred):\n",
        "    n = y_true.size(0)  # Number of observations\n",
        "    k = y_pred.size(1) if len(y_pred.size()) > 1 else 1  # Number of predictors\n",
        "    r2 = r2_score(y_true.cpu().numpy(), y_pred.cpu().numpy())\n",
        "    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - k - 1)\n",
        "    return adj_r2\n",
        "\n",
        "# Create a custom metric for FastAI\n",
        "class AdjustedR2Score(Metric):\n",
        "    def __init__(self):\n",
        "        self.preds = []\n",
        "        self.targets = []\n",
        "\n",
        "    def reset(self):\n",
        "        self.preds, self.targets = [], []\n",
        "\n",
        "    def accumulate(self, learn):\n",
        "        preds, targets = learn.pred, learn.y\n",
        "        # Check if preds and targets are not empty\n",
        "        if len(preds) > 0 and len(targets) > 0:\n",
        "            self.preds.append(preds)\n",
        "            self.targets.append(targets)\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        # Check if preds and targets are not empty before concatenating\n",
        "        if self.preds and self.targets:\n",
        "            preds = torch.cat(self.preds)\n",
        "            targets = torch.cat(self.targets)\n",
        "            return adjusted_r2_score(targets, preds)\n",
        "        else:\n",
        "            return 0.0"
      ],
      "metadata": {
        "id": "hCMoSSnSNJAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to reduce dataset while maintaining class balance\n",
        "def reduce_dataset(dataset, target_size):\n",
        "    targets = np.array(dataset.targets)\n",
        "    classes, class_counts = np.unique(targets, return_counts=True)\n",
        "    samples_per_class = target_size // len(classes)\n",
        "\n",
        "    reduced_indices = []\n",
        "    for cls in classes:\n",
        "        cls_indices = np.where(targets == cls)[0]\n",
        "        sampled_indices = np.random.choice(cls_indices, samples_per_class, replace=False)\n",
        "        reduced_indices.extend(sampled_indices)\n",
        "\n",
        "    reduced_dataset = torch.utils.data.Subset(dataset, reduced_indices)\n",
        "    return reduced_dataset\n",
        "\n",
        "# Verify class balance\n",
        "def verify_class_balance(dataset):\n",
        "    targets = np.array(dataset.dataset.targets)[dataset.indices]\n",
        "    classes, class_counts = np.unique(targets, return_counts=True)\n",
        "    for cls, count in zip(classes, class_counts):\n",
        "        print(f'Class {cls}: {count} samples')\n"
      ],
      "metadata": {
        "id": "ePTHkhQEdGcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_models(model1, model2):\n",
        "    for param1, param2 in zip(model1.state_dict().items(), model2.state_dict().items()):\n",
        "        if not torch.equal(param1[1], param2[1]):\n",
        "            return False\n",
        "    return True"
      ],
      "metadata": {
        "id": "MwHGz92NKECf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.init as init\n",
        "\n",
        "def init_weights_glorot(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "        init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            init.constant_(m.bias, 0)"
      ],
      "metadata": {
        "id": "B3YB0PLCRklM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_learners_training(learners, names = [], title='Accuracy, Training and Validation Loss', metric=['accuracy'], lim=1):\n",
        "    \"\"\"\n",
        "    Plots the training and validation loss curves for an array of fastai learners.\n",
        "\n",
        "    Parameters:\n",
        "    - learners: list of fastai Learner objects\n",
        "    - metric: 'loss' to plot loss curves (default), or 'metric' to plot a specific metric\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 9))\n",
        "    colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
        "    max_epochs = max(len(learn.recorder.values) for learn in learners)\n",
        "    for idx, learn in enumerate(learners):\n",
        "        color = colors[idx % len(colors)]  # Cycle through colors\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        metrics = [[] for _ in range(len(metric))]\n",
        "        for epoch in range(len(learn.recorder.values)):\n",
        "          train_losses.append(learn.recorder.values[epoch][0])\n",
        "          val_losses.append(learn.recorder.values[epoch][1])\n",
        "          for met in range(len(metric)):\n",
        "            metrics[met].append(learn.recorder.values[epoch][2+met])\n",
        "          label_suffix = ''\n",
        "        label_prefix = f' - {names[idx]}' if len(names) > 0 else f'Learner {idx + 1}'\n",
        "        # Plot training loss\n",
        "        plt.plot(train_losses, label=label_prefix + ' - Training Loss', color=color)\n",
        "        # Plot validation loss\n",
        "        plt.plot(val_losses, label=label_prefix + ' - Validation Loss', linestyle='--', color=color)\n",
        "\n",
        "        # Plot metrics\n",
        "        lines=['-.', ':']\n",
        "        for idx, met in enumerate(metric):\n",
        "          plt.plot(metrics[idx], label=label_prefix + f' - {met}', linestyle=lines[idx], color=color)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.xlim(0, max_epochs-1)\n",
        "    plt.ylim(0,lim)\n",
        "\n",
        "    # Adjust the plot area to make room for the legend\n",
        "    plt.subplots_adjust(right=0.75)\n",
        "\n",
        "    # Move legend outside of the plot\n",
        "    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "lLpyPnoS9POe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_mh_mlp_training(train_losses, val_losses, accuracies,  layers, names=[], title='Training Metrics', metric = ['Accuracy'], lim=1,f1score=None):\n",
        "    \"\"\"\n",
        "    Plots the training and validation loss curves along with accuracy for an array of models.\n",
        "\n",
        "    Parameters:\n",
        "    - train_losses: list of lists, where each sublist contains the training loss per epoch for a model\n",
        "    - val_losses: list of lists, where each sublist contains the validation loss per epoch for a model\n",
        "    - train_accuracies: list of lists, where each sublist contains the training accuracy per epoch for a model\n",
        "    - val_accuracies: list of lists, where each sublist contains the validation accuracy per epoch for a model\n",
        "    - names: list of names for the models (default is an empty list)\n",
        "    - title: title of the plot (default is 'Training Metrics')\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    max_epochs = max(len(train_losses[layer]) for layer in layers)\n",
        "    colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
        "\n",
        "    for idx,layer in enumerate(train_losses.keys()):\n",
        "        color = colors[idx % len(colors)]  # Cycle through colors\n",
        "        label_suffix = ''\n",
        "        label_prefix = f' - {names[idx]}' if len(names) > 0 else f'Model {idx + 1}'\n",
        "\n",
        "\n",
        "\n",
        "        # Plot training loss\n",
        "        plt.plot(train_losses[layer], label=label_prefix + ' - Training Loss', color=color)\n",
        "\n",
        "        # Plot validation loss\n",
        "        plt.plot(val_losses[layer], label=label_prefix + ' - Validation Loss', linestyle='--', color=color)\n",
        "\n",
        "        # Plot balanced accuracy\n",
        "        plt.plot(accuracies[layer], label=label_prefix + ' - Balanced Accuracy', linestyle='-.', color=color)\n",
        "\n",
        "        # Plot F1 Scores\n",
        "       #plt.plot(f1score[layer], label=label_prefix + ' - F1 Score', linestyle=':', color=color)\n",
        "\n",
        "\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Metrics')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.xlim(0, max_epochs-1)\n",
        "    plt.ylim(0,lim)\n",
        "    # Adjust the plot area to make room for the legend\n",
        "    plt.subplots_adjust(right=0.75)\n",
        "\n",
        "    # Move legend outside of the plot\n",
        "    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "XWftWFUwk8ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get params from a model in a numpy array\n",
        "\n",
        "def get_params_from_model(model):\n",
        "    return np.concatenate([v.cpu().detach().numpy().ravel() for v in model.parameters()])\n",
        "\n",
        "\n",
        "\n",
        "#The params are set into the global model to compute the loss.\n",
        "def set_params_to_model(params, model):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    offset = 0\n",
        "    for param in model.parameters():\n",
        "        param_size = param.numel()\n",
        "        param.data = torch.tensor(params[offset:offset + param_size], dtype=param.data.dtype).view(param.size()).to(device)\n",
        "        param.requires_grad = True\n",
        "        offset += param_size\n",
        "\n",
        "#Get the valid loss\n",
        "def err_param_valid(params, model, dls, task='clas'):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)  # Move model to the device\n",
        "\n",
        "\n",
        "    set_params_to_model(params, model)\n",
        "\n",
        "    model.eval()\n",
        "    #Option 1: calculate the error per each batch and make the mean\n",
        "\n",
        "    loss =0.0\n",
        "\n",
        "    if task == 'clas':\n",
        "      func = F.cross_entropy\n",
        "    elif task == 'reg':\n",
        "      func = F.mse_loss\n",
        "\n",
        "    #If i have categorical, continius and the target\n",
        "    if len(dls.one_batch()) > 2:\n",
        "      for batch in dls.valid:\n",
        "        # Handle the case where there are no continuous features\n",
        "        x_cont = batch[1].to(device) if len(batch) > 1 and batch[1] is not None else None\n",
        "        pred = model(batch[0].to(device), x_cont)  # Pass both categorical and continuous features (if any)\n",
        "        pred = pred.squeeze().clone().detach() if task == 'reg' else pred.clone().detach()\n",
        "        target = batch[2].to(device).squeeze().long().clone().detach()\n",
        "        loss += func(pred, target)\n",
        "\n",
        "\n",
        "    #If not\n",
        "    else:\n",
        "      for batch in dls.valid:\n",
        "        # Handle the case where there are no continuous features\n",
        "\n",
        "        pred = model(batch[0].to(device))  # Pass both categorical and continuous features (if any)\n",
        "        pred = pred.squeeze().clone().detach() if task == 'reg' else pred.clone().detach()\n",
        "        target = batch[1].to(device).squeeze().long().clone().detach()\n",
        "        loss += func(pred, target)\n",
        "\n",
        "    loss/=len(dls.valid)\n",
        "\n",
        "\n",
        "    return loss.mean().item()\n",
        "\n",
        "\n",
        "\n",
        "#Get de train loss\n",
        "def err_param(params, model, dls, mode='train', task='clas'):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)  # Move model to the device\n",
        "    set_params_to_model(params, model)\n",
        "\n",
        "    if mode == 'train':\n",
        "      model.train()\n",
        "    else:\n",
        "      model.eval()\n",
        "    #Option 1: calculate the error per each batch and make the mean\n",
        "\n",
        "    if task == 'clas':\n",
        "      func = F.cross_entropy\n",
        "    elif task == 'reg':\n",
        "      func = F.mse_loss\n",
        "\n",
        "\n",
        "    loss =0.0\n",
        "    it = dls.train if hasattr(dls,'train') else dls\n",
        "    #If i have categorical, continius and the target\n",
        "    if len(dls.one_batch()) > 2:\n",
        "      for batch in it:\n",
        "        # Handle the case where there are no continuous features\n",
        "        x_cont = batch[1].to(device) if len(batch) > 1 and batch[1] is not None else None\n",
        "        pred = model(batch[0].to(device), x_cont)  # Pass both categorical and continuous features (if any)\n",
        "        pred = pred.squeeze().clone().detach() if task == 'reg' else pred.clone().detach()\n",
        "        target = batch[2].to(device).squeeze().long().clone().detach()\n",
        "        loss += func(pred, target)\n",
        "\n",
        "\n",
        "    #If not\n",
        "    else:\n",
        "      for batch in it:\n",
        "        # Handle the case where there are no continuous features\n",
        "\n",
        "        pred = model(batch[0].to(device))  # Pass both categorical and continuous features (if any)\n",
        "        pred = pred.squeeze().clone().detach() if task == 'reg' else pred.clone().detach()\n",
        "        target = batch[1].to(device).squeeze().long().clone().detach()\n",
        "        loss += func(pred, target)\n",
        "\n",
        "    loss/=len(it)\n",
        "\n",
        "\n",
        "    #If i only have one element it wont affect, but if i have more its necessary\n",
        "    return loss.mean().item()\n",
        "\n",
        "\n",
        "\n",
        "def err_param_w_model(model, dls, mode='train', task='clas'):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)  # Move model to the device\n",
        "\n",
        "\n",
        "    if mode == 'train':\n",
        "      model.train()\n",
        "    else:\n",
        "      model.eval()\n",
        "\n",
        "    if task == 'clas':\n",
        "      func = F.cross_entropy\n",
        "    elif task == 'reg':\n",
        "      func = F.mse_loss\n",
        "\n",
        "\n",
        "    #Option 1: calculate the error per each batch and make the mean\n",
        "\n",
        "    loss =0.0\n",
        "\n",
        "    #If i have categorical, continius and the target\n",
        "    it = dls.train if hasattr(dls,'train') else dls\n",
        "    if len(dls.one_batch()) > 2:\n",
        "\n",
        "\n",
        "      for batch in it:\n",
        "        # Handle the case where there are no continuous features\n",
        "        x_cont = batch[1].to(device) if len(batch) > 1 and batch[1] is not None else None\n",
        "        pred = model(batch[0].to(device), x_cont)  # Pass both categorical and continuous features (if any)\n",
        "        pred = pred.squeeze().clone().detach() if task == 'reg' else pred.clone().detach()\n",
        "        target = batch[2].to(device).squeeze().long().clone().detach()\n",
        "        loss += func(pred, target)\n",
        "\n",
        "\n",
        "    #If not\n",
        "    else:\n",
        "      for batch in it:\n",
        "        # Handle the case where there are no continuous features\n",
        "\n",
        "        pred = model(batch[0].to(device))  # Pass both categorical and continuous features (if any)\n",
        "        pred = pred.squeeze().clone().detach() if task == 'reg' else pred.clone().detach()\n",
        "        target = batch[1].to(device).squeeze().long().clone().detach()\n",
        "        loss += func(pred, target)\n",
        "\n",
        "    loss/=len(it)\n",
        "\n",
        "\n",
        "    #If i only have one element it wont affect, but if i have more its necessary\n",
        "    return loss.mean().item()\n",
        "\n",
        "def err_param_shade2(params, opts):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model, dls, task = opts\n",
        "    model = model.to(device)  # Move model to the device\n",
        "    set_params_to_model(params, model)\n",
        "\n",
        "    if task == 'clas':\n",
        "      func = F.cross_entropy\n",
        "    elif task == 'reg':\n",
        "      func = F.mse_loss\n",
        "\n",
        "    model.train()\n",
        "    #Option 1: calculate the error per each batch and make the mean\n",
        "\n",
        "    loss =0.0\n",
        "\n",
        "    #If i have categorical, continius and the target\n",
        "    if len(dls.one_batch()) > 2:\n",
        "      for batch in dls.train:\n",
        "        # Handle the case where there are no continuous features\n",
        "        x_cont = batch[1].to(device) if len(batch) > 1 and batch[1] is not None else None\n",
        "        pred = model(batch[0].to(device), x_cont)  # Pass both categorical and continuous features (if any)\n",
        "        pred = pred.squeeze().clone().detach() if task == 'reg' else pred.clone().detach()\n",
        "        target = batch[2].to(device).squeeze().long().clone().detach()\n",
        "        loss += func(pred, target)\n",
        "\n",
        "\n",
        "    #If not\n",
        "    else:\n",
        "      for batch in dls.train:\n",
        "        # Handle the case where there are no continuous features\n",
        "\n",
        "        pred = model(batch[0].to(device))  # Pass both categorical and continuous features (if any)\n",
        "        pred = pred.squeeze().clone().detach() if task == 'reg' else pred.clone().detach()\n",
        "        target = batch[1].to(device).squeeze().long().clone().detach()\n",
        "        loss += func(pred, target)\n",
        "\n",
        "    loss/=len(dls.train)\n",
        "\n",
        "\n",
        "    #If i only have one element it wont affect, but if i have more its necessary\n",
        "    return loss.mean().item()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#def test_err_param(params, model, batch):\n",
        "#    set_params_to_model(params, model)\n",
        "#\n",
        "#    model.train()\n",
        "#    #Option 1: calculate the error per each batch and make the mean\n",
        "#\n",
        "#    # Handle the case where there are no continuous features\n",
        "#    if len(batch) > 2:\n",
        "#      x_cont = batch[1] if len(batch) > 1 and batch[1] is not None else None\n",
        "#      pred = model(batch[0], x_cont)  # Pass both categorical and continuous features (if any)\n",
        "#      pred = torch.tensor(pred.clone().detach())#.to('cuda')\n",
        "#      target = torch.tensor(batch[2].squeeze().long().clone().detach())#.to('cuda')\n",
        "#\n",
        "#    else:\n",
        "#      pred = model(batch[0])  # Pass both categorical and continuous features (if any)\n",
        "#      pred = torch.tensor(pred.clone().detach())#.to('cuda')\n",
        "#      target = torch.tensor(batch[1].squeeze().long().clone().detach())#.to('cuda')\n",
        "#\n",
        "#    loss = F.cross_entropy(pred, target)\n",
        "#\n",
        "#\n",
        "#\n",
        "#    return loss.mean().item()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VrGkcr8MAIFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BUSQUEDA LOCAL CON LBFGS CON LIBRERIA SCIPY\n",
        "from scipy.optimize import minimize\n",
        "from functools import partial\n",
        "def get_gradient(ind, model, dls, task):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)  # Move model to the device\n",
        "\n",
        "    set_params_to_model(ind, model)\n",
        "    model.train()  # Put the model in training mode\n",
        "    batch = dls.train.one_batch()\n",
        "\n",
        "    if task == 'clas':\n",
        "      func = F.cross_entropy\n",
        "    elif task == 'reg':\n",
        "      func = F.mse_loss\n",
        "\n",
        "\n",
        "    if len(batch) > 2:\n",
        "      x_cont = batch[1].to(device) if len(batch) > 1 and batch[1] is not None else None\n",
        "      pred = model(batch[0].to(device), x_cont)  # Make predictions\n",
        "      pred = pred.as_subclass(torch.Tensor).clone()\n",
        "      target = batch[2].to(device).squeeze().long().as_subclass(torch.Tensor).clone()\n",
        "\n",
        "    else:\n",
        "      pred = model(batch[0].to(device))  # Make predictions\n",
        "      # Convert predictions and targets to torch.Tensor subclass\n",
        "      pred = pred.as_subclass(torch.Tensor).clone()\n",
        "      target = batch[1].to(device).squeeze().long().as_subclass(torch.Tensor).clone()\n",
        "\n",
        "    # Calculate the loss using cross_entropy\n",
        "    loss = func(pred, target)\n",
        "    loss.backward()  # Calculate gradients\n",
        "\n",
        "    # Extract gradients into a list\n",
        "    gradients = [param.grad.clone().detach().cpu().numpy().ravel() for param in model.parameters()]\n",
        "\n",
        "    # Concatenate gradients into a single 1D array\n",
        "    return loss.mean().item(), np.concatenate(gradients)\n",
        "\n",
        "\n",
        "def get_gradient_full(ind, model, dls, task):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)  # Move model to the device\n",
        "    set_params_to_model(ind, model)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    if task == 'clas':\n",
        "      func = F.cross_entropy\n",
        "    elif task == 'reg':\n",
        "      func = F.mse_loss\n",
        "\n",
        "\n",
        "    model.zero_grad()\n",
        "    #Option 1: calculate the error per each batch and make the mean\n",
        "\n",
        "    loss =0.0\n",
        "\n",
        "    #If i have categorical, continius and the target\n",
        "    if len(dls.one_batch()) > 2:\n",
        "      for batch in dls.train:\n",
        "        # Handle the case where there are no continuous features\n",
        "        x_cont = batch[1].to(device) if len(batch) > 1 and batch[1] is not None else None\n",
        "        pred = model(batch[0].to(device), x_cont)  # Pass both categorical and continuous features (if any)\n",
        "        pred = pred.squeeze().clone() if task == 'reg' else pred.clone()\n",
        "        target = batch[2].to(device).squeeze().float().clone() if task == 'reg' else batch[2].to(device).squeeze().long().clone()\n",
        "        loss += func(pred, target).float()\n",
        "\n",
        "\n",
        "    #If not\n",
        "    else:\n",
        "      for batch in dls.train:\n",
        "        # Handle the case where there are no continuous features\n",
        "\n",
        "        pred = model(batch[0].to(device))  # Pass both categorical and continuous features (if any)\n",
        "        pred = pred.clone()\n",
        "        target = batch[1].to(device).squeeze().float().clone() if task == 'reg' else batch[1].to(device).squeeze().long().clone()\n",
        "        loss += func(pred, target)\n",
        "\n",
        "    loss/=len(dls.train)\n",
        "\n",
        "    loss.backward()  # Calculate gradients\n",
        "\n",
        "    # Extract gradients into a list\n",
        "    gradients = [param.grad.clone().detach().cpu().numpy().ravel() for param in model.parameters()]\n",
        "\n",
        "    # Concatenate gradients into a single 1D array\n",
        "    return loss.item(), np.concatenate(gradients)\n",
        "\n",
        "def LBFGS_SCIPY(ind, model, dls, task='clas'):\n",
        "    get_grad = partial(get_gradient, model=model, dls=dls, task=task)\n",
        "    result = minimize(get_grad, ind, jac=True, method='L-BFGS-B')\n",
        "    return result.x, result.fun, result.nfev, result.njev\n",
        "\n",
        "def LBFGS_SCIPY_full(ind, model, dls, max=5, task='clas'):\n",
        "  get_grad = partial(get_gradient_full, model=model, dls=dls, task=task)\n",
        "  result = minimize(get_grad, ind, jac=True, method='L-BFGS-B', options={'maxiter': max})\n",
        "  return result.x, result.fun, result.nfev, result.njev\n"
      ],
      "metadata": {
        "id": "fX38ayOzE1mD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable\n",
        "!pip install /content/drive/MyDrive/pyade-master\n",
        "import pyade\n",
        "\n",
        "import pyade.shade\n",
        "\n",
        "algorithm = pyade.shade\n",
        "\n",
        "def SHADE_ej(population, fitness, pop_size, max_evals, dim, dls, prevm_cr=0, prevm_f=0, k=0, model=None, task='clas'):\n",
        "\n",
        "  population = np.array(population).reshape(pop_size, dim)\n",
        "  fitness = np.array(fitness)\n",
        "\n",
        "  params = algorithm.get_default_params(dim=dim)\n",
        "  # We define the boundaries of the variables\n",
        "  params['bounds'] = np.array([[-50, 50]] * dim)\n",
        "\n",
        "  # We indicate the function we want to minimize\n",
        "  params['func'] = err_param_shade\n",
        "\n",
        "  params['population_size']=pop_size\n",
        "\n",
        "\n",
        "\n",
        "  params['seed']=42\n",
        "\n",
        "  params['opts']=[model, dls, task]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  params['init_pop']=population\n",
        "\n",
        "  params['max_evals']=max_evals\n",
        "\n",
        "\n",
        "  params['prevm_cr']=prevm_cr\n",
        "\n",
        "  params['prevm_f']=prevm_f\n",
        "\n",
        "  params['k']=k\n",
        "\n",
        "  params['fitness']=fitness\n",
        "\n",
        "  return algorithm.apply(**params)"
      ],
      "metadata": {
        "id": "gMPvAEaTEl-G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda4d987-4615-4a62-9b99-91347105eae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ./drive/MyDrive/pyade-master\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pyade-python\n",
            "  Building wheel for pyade-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyade-python: filename=pyade_python-1.0-py3-none-any.whl size=29072 sha256=07541f727ee91604b9a3adafa755333834bb36844754f5087783c6a055049262\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/b8/51/ff2ebef24405d3ef46ccb451f962b374b2745f263fb000c434\n",
            "Successfully built pyade-python\n",
            "Installing collected packages: pyade-python\n",
            "Successfully installed pyade-python-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Algoritmo SHADE-ILS\n",
        "from collections import deque\n",
        "\n",
        "import copy\n",
        "\n",
        "def reset_weights(m):\n",
        "    if hasattr(m, 'reset_parameters'):\n",
        "        m.reset_parameters()\n",
        "\n",
        "\n",
        "\n",
        "def SHADE_ILS(population, fit, max_evals, max_shade, dls, learner, model,  max_ls=5, func=None, task='clas'):\n",
        "  LEARN=learner\n",
        "\n",
        "\n",
        "  historic_fitness=[]\n",
        "  historic_best_solution=[]\n",
        "\n",
        "  population_size = len(population)\n",
        "  evals = population_size\n",
        "  max_evals = max_evals\n",
        "  max_evals_SHADE = max_shade\n",
        "  prevm_cr=0\n",
        "  prevm_f=0\n",
        "  k=0\n",
        "  must_restart=False\n",
        "  improq = deque([1,1,1], maxlen=3)\n",
        "\n",
        "  #Initialize the population\n",
        "  #population = [get_params_from_model(model) for _ in range(len(population))]\n",
        "  fitness = fit\n",
        "  size_ind=len(population[0])\n",
        "\n",
        "  #Select the best\n",
        "\n",
        "  current_best_fitness = min(fitness)\n",
        "  current_best_index = fitness.index(current_best_fitness)\n",
        "  current_best = population[current_best_index]\n",
        "\n",
        "  historic_fitness.append(current_best_fitness)\n",
        "  historic_best_solution.append(current_best)\n",
        "\n",
        "  temp_current_best, _, e1, e2 = LBFGS_SCIPY_full(current_best, model, dls, max=max_ls, task=task)\n",
        "  evals += e1\n",
        "  temp_current_best_fitness=err_param(temp_current_best, model, dls, task=task)\n",
        "\n",
        "  improq.append((current_best_fitness-temp_current_best_fitness)/current_best_fitness)\n",
        "\n",
        "  if temp_current_best_fitness < current_best_fitness:\n",
        "    current_best_fitness = temp_current_best_fitness\n",
        "    current_best = temp_current_best\n",
        "\n",
        "\n",
        "  population[current_best_index] = current_best\n",
        "  fitness[current_best_index] = current_best_fitness\n",
        "\n",
        "  best_solution=current_best\n",
        "  best_fitness=current_best_fitness\n",
        "  historic_fitness.append(best_fitness)\n",
        "  historic_best_solution.append(best_solution)\n",
        "\n",
        "  while evals < max_evals:\n",
        "    print(\"eo\")\n",
        "    population, fitness, prevm_cr, prevm_f, k , _, _= SHADE_ej(population, fitness, population_size, max_evals_SHADE, size_ind, dls,  prevm_cr=prevm_cr, prevm_f=prevm_f, k=k, model=model, task=task)\n",
        "    evals += max_evals_SHADE\n",
        "    temp_current_best_fitness= min(fitness)\n",
        "\n",
        "    improq.append((current_best_fitness-temp_current_best_fitness)/current_best_fitness)\n",
        "\n",
        "    if temp_current_best_fitness < current_best_fitness:\n",
        "      current_best_fitness = temp_current_best_fitness\n",
        "      current_best_index = list(fitness).index(current_best_fitness)\n",
        "      current_best = population[current_best_index]\n",
        "\n",
        "\n",
        "\n",
        "    print(improq)\n",
        "    if np.all(np.array(improq)<0.05):\n",
        "      must_restart=True\n",
        "\n",
        "    #Choose the LS method to apply this iteration based on improvement\n",
        "\n",
        "    temp_current_best, _, e1, e2 = LBFGS_SCIPY_full(current_best, model, dls, max=max_ls, task=task)\n",
        "    evals += e1\n",
        "    temp_current_best_fitness=err_param(temp_current_best, model, dls, task=task)\n",
        "\n",
        "    improq.append((current_best_fitness-temp_current_best_fitness)/current_best_fitness)\n",
        "\n",
        "    #if temp_current_best_fitness < current_best_fitness:\n",
        "    current_best_fitness = temp_current_best_fitness\n",
        "    current_best = temp_current_best\n",
        "\n",
        "\n",
        "    print(improq)\n",
        "    if np.all(np.array(improq)<0.05):\n",
        "      must_restart=True\n",
        "\n",
        "    population[current_best_index] = current_best\n",
        "    fitness[current_best_index] = current_best_fitness\n",
        "\n",
        "    #Update the probability to apply LS in next iterations\n",
        "\n",
        "    if current_best_fitness < best_fitness:\n",
        "      best_fitness = current_best_fitness\n",
        "      best_solution = copy.deepcopy(current_best)\n",
        "\n",
        "    historic_fitness.append(best_fitness)\n",
        "    historic_best_solution.append(best_solution)\n",
        "\n",
        "    if must_restart:\n",
        "      random_index = np.random.choice(population_size)\n",
        "      sol = population[random_index]\n",
        "      sol += np.random.normal(0, 0.5, len(sol))\n",
        "      k=0\n",
        "      prevm_cr=0\n",
        "      prevm_f=0\n",
        "      population=[]\n",
        "      for _ in range(population_size-1):\n",
        "        model_copy = copy.deepcopy(model)\n",
        "\n",
        "        # Reset the weights of the model copy\n",
        "        model_copy.apply(init_weights_glorot)\n",
        "\n",
        "        # Add the reinitialized model to the population list\n",
        "        population.append(get_params_from_model(model_copy))\n",
        "\n",
        "      fitness = [err_param(elem, model, dls,task=task) for elem in population]\n",
        "      evals += population_size\n",
        "      population.append(sol)\n",
        "      current_best_fitness=err_param(sol, model, dls,task=task)\n",
        "      current_best=sol\n",
        "      fitness.append(current_best_fitness)\n",
        "      improq=deque([1,1,1], maxlen=3)\n",
        "\n",
        "      print(\"Reiniciado\")\n",
        "      must_restart=False\n",
        "\n",
        "    print(evals)\n",
        "  return best_solution, best_fitness, historic_fitness, historic_best_solution\n",
        "\n",
        "\n",
        "\n",
        "def SHADE_GD(population, fit, max_evals, max_shade, dls, learner, model, task='clas'):\n",
        "  LEARN=learner\n",
        "\n",
        "\n",
        "  historic_fitness=[]\n",
        "  historic_best_solution=[]\n",
        "\n",
        "  population_size = len(population)\n",
        "  evals = population_size\n",
        "  max_evals = max_evals\n",
        "  max_evals_SHADE = max_shade\n",
        "  prevm_cr=0\n",
        "  prevm_f=0\n",
        "  k=0\n",
        "  must_restart=False\n",
        "  improq = deque([1,1,1], maxlen=3)\n",
        "\n",
        "  app_gener= max_evals/max_evals_SHADE\n",
        "  gen=0\n",
        "\n",
        "  #Initialize the population\n",
        "  #population = [get_params_from_model(model) for _ in range(len(population))]\n",
        "  fitness = fit\n",
        "  evals += population_size\n",
        "  size_ind=len(population[0])\n",
        "\n",
        "  #Select the best\n",
        "\n",
        "  current_best_fitness = min(fitness)\n",
        "  current_best_index = fitness.index(current_best_fitness)\n",
        "  current_best = population[current_best_index]\n",
        "\n",
        "  population[current_best_index] = current_best\n",
        "  fitness[current_best_index] = current_best_fitness\n",
        "\n",
        "  best_solution=current_best\n",
        "  best_fitness=current_best_fitness\n",
        "  historic_fitness.append(best_fitness)\n",
        "  historic_best_solution.append(best_solution)\n",
        "\n",
        "  while evals < max_evals:\n",
        "    gen+=1\n",
        "    print(\"eo\")\n",
        "    population, fitness, prevm_cr, prevm_f, k, _, _ = SHADE_ej(population, fitness, population_size, max_evals_SHADE, size_ind, dls,  prevm_cr=prevm_cr, prevm_f=prevm_f, k=k, model=model, task=task)\n",
        "    evals += max_evals_SHADE\n",
        "    temp_current_best_fitness= min(fitness)\n",
        "\n",
        "    improq.append((current_best_fitness-temp_current_best_fitness)/current_best_fitness)\n",
        "\n",
        "    if temp_current_best_fitness < current_best_fitness:\n",
        "      current_best_fitness = temp_current_best_fitness\n",
        "      current_best_index = list(fitness).index(current_best_fitness)\n",
        "      current_best = population[current_best_index]\n",
        "\n",
        "\n",
        "\n",
        "    print(improq)\n",
        "    if np.all(np.array(improq)<0.05):\n",
        "      must_restart=True\n",
        "\n",
        "    #Choose the LS method to apply this iteration based on improvement\n",
        "\n",
        "\n",
        "    if gen%5==0:\n",
        "      ind = np.random.randint(0, population_size)\n",
        "      set_params_to_model(population[ind], LEARN.model)\n",
        "      try:\n",
        "        lr=LEARN.lr_find()\n",
        "\n",
        "      except E as e:\n",
        "        print(f\"Ha fallado {e} con ind={ind}, current_best_fitness={current_best_fitness} y best_fitness={best_fitness}\")\n",
        "\n",
        "      finally:\n",
        "        LEARN.fit(1, lr)\n",
        "      population[ind]=get_params_from_model(LEARN.model)\n",
        "      fit=LEARN.recorder.values[-1][0]\n",
        "      fitness[ind]=fit\n",
        "      temp_current_best_fitness=fit\n",
        "      temp_current_best=population[ind]\n",
        "      evals+=1\n",
        "\n",
        "\n",
        "      improq.append((current_best_fitness-temp_current_best_fitness)/current_best_fitness)\n",
        "\n",
        "      #if temp_current_best_fitness < current_best_fitness:\n",
        "      current_best_fitness = temp_current_best_fitness\n",
        "      current_best = temp_current_best\n",
        "\n",
        "\n",
        "    print(improq)\n",
        "    if np.all(np.array(improq)<0.05):\n",
        "      must_restart=True\n",
        "\n",
        "\n",
        "\n",
        "    #Update the probability to apply LS in next iterations\n",
        "\n",
        "    if current_best_fitness < best_fitness:\n",
        "      best_fitness = np.copy(current_best_fitness)\n",
        "      best_solution = np.copy(current_best)\n",
        "\n",
        "    historic_fitness.append(best_fitness if np.isscalar(best_fitness) else float(best_fitness))\n",
        "    historic_best_solution.append(best_solution)\n",
        "\n",
        "    if must_restart:\n",
        "      random_index = np.random.choice(population_size)\n",
        "      sol = population[random_index]\n",
        "      sol += np.random.normal(0, 0.3, len(sol))\n",
        "\n",
        "      k=0\n",
        "      prevm_cr=0\n",
        "      prevm_f=0\n",
        "      population=[]\n",
        "      for _ in range(population_size-1):\n",
        "        model_copy = copy.deepcopy(model)\n",
        "\n",
        "        # Reset the weights of the model copy\n",
        "        model_copy.apply(init_weights_glorot)\n",
        "\n",
        "        # Add the reinitialized model to the population list\n",
        "        population.append(get_params_from_model(model_copy))\n",
        "\n",
        "      fitness = [err_param(elem, model, dls,task=task) for elem in population]\n",
        "      evals += population_size\n",
        "      population.append(sol)\n",
        "      current_best_fitness=err_param(sol, model, dls, task=task)\n",
        "      current_best=sol\n",
        "      fitness.append(current_best_fitness)\n",
        "      improq=deque([1,1,1], maxlen=3)\n",
        "\n",
        "      print(\"Reiniciado\")\n",
        "      must_restart=False\n",
        "\n",
        "  return best_solution, best_fitness, historic_fitness, historic_best_solution\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def SHADE_ILS_GD(population, fit, max_evals, max_shade, dls, learner, model, max_ls=5, task='clas'):\n",
        "  LEARN=learner\n",
        "\n",
        "\n",
        "  historic_fitness=[]\n",
        "  historic_best_solution=[]\n",
        "\n",
        "  population_size = len(population)\n",
        "  evals = population_size\n",
        "  max_evals = max_evals\n",
        "  max_evals_SHADE = max_shade\n",
        "  prevm_cr=0\n",
        "  prevm_f=0\n",
        "  k=0\n",
        "  lr=0.001\n",
        "  must_restart=False\n",
        "  improq = deque([1,1,1], maxlen=3)\n",
        "\n",
        "  app_gener= max_evals/max_evals_SHADE\n",
        "  gen=0\n",
        "\n",
        "  #Initialize the population\n",
        "  #population = [get_params_from_model(model) for _ in range(len(population))]\n",
        "  population = population\n",
        "  fitness = fit\n",
        "  evals += population_size\n",
        "  size_ind=len(population[0])\n",
        "\n",
        "  #Select the best\n",
        "\n",
        "  current_best_fitness = min(fitness)\n",
        "  current_best_index = fitness.index(current_best_fitness)\n",
        "  current_best = population[current_best_index]\n",
        "\n",
        "  population[current_best_index] = current_best\n",
        "  fitness[current_best_index] = current_best_fitness\n",
        "\n",
        "  best_solution=current_best\n",
        "  best_fitness=current_best_fitness\n",
        "  historic_fitness.append(best_fitness)\n",
        "  historic_best_solution.append(best_solution)\n",
        "\n",
        "  temp_current_best, _, e1, e2 = LBFGS_SCIPY_full(current_best, model, dls, max=max_ls, task=task)\n",
        "  evals += e1\n",
        "  temp_current_best_fitness=err_param(temp_current_best, model, dls,task=task)\n",
        "\n",
        "  improq.append((current_best_fitness-temp_current_best_fitness)/current_best_fitness)\n",
        "\n",
        "  if temp_current_best_fitness < current_best_fitness:\n",
        "    current_best_fitness = temp_current_best_fitness\n",
        "    current_best = temp_current_best\n",
        "\n",
        "\n",
        "  print(improq)\n",
        "  if np.all(np.array(improq)<0.05):\n",
        "    must_restart=True\n",
        "\n",
        "  population[current_best_index] = current_best\n",
        "  fitness[current_best_index] = current_best_fitness\n",
        "\n",
        "\n",
        "  historic_fitness.append(current_best_fitness)\n",
        "  historic_best_solution.append(current_best)\n",
        "\n",
        "  while evals < max_evals:\n",
        "    gen+=1\n",
        "    print(\"eo\")\n",
        "    population, fitness, prevm_cr, prevm_f, k, _, _ = SHADE_ej(population, fitness, population_size, max_evals_SHADE, size_ind, dls,  prevm_cr=prevm_cr, prevm_f=prevm_f, k=k, model=model, task=task)\n",
        "    evals += max_evals_SHADE\n",
        "    temp_current_best_fitness= min(fitness)\n",
        "\n",
        "    improq.append((current_best_fitness-temp_current_best_fitness)/current_best_fitness)\n",
        "\n",
        "    if temp_current_best_fitness < current_best_fitness:\n",
        "      current_best_fitness = temp_current_best_fitness\n",
        "      current_best_index = list(fitness).index(current_best_fitness)\n",
        "      current_best = population[current_best_index]\n",
        "\n",
        "\n",
        "\n",
        "    print(improq)\n",
        "    if np.all(np.array(improq)<0.05):\n",
        "      must_restart=True\n",
        "\n",
        "\n",
        "    temp_current_best, _, e1, e2 = LBFGS_SCIPY_full(current_best, model, dls, max=max_ls, task=task)\n",
        "    evals += e1\n",
        "    temp_current_best_fitness=err_param(temp_current_best, model, dls, task=task)\n",
        "\n",
        "    improq.append((current_best_fitness-temp_current_best_fitness)/current_best_fitness)\n",
        "\n",
        "    #if temp_current_best_fitness < current_best_fitness:\n",
        "    current_best_fitness = temp_current_best_fitness\n",
        "    current_best = temp_current_best\n",
        "\n",
        "\n",
        "    print(improq)\n",
        "    if np.all(np.array(improq)<0.05):\n",
        "      must_restart=True\n",
        "\n",
        "    population[current_best_index] = current_best\n",
        "    fitness[current_best_index] = current_best_fitness\n",
        "\n",
        "\n",
        "    if gen%5==0:\n",
        "      ind = np.random.randint(0, population_size)\n",
        "      set_params_to_model(population[ind], LEARN.model)\n",
        "\n",
        "      try:\n",
        "        lr=LEARN.lr_find()\n",
        "\n",
        "      except E as e:\n",
        "        print(f\"Ha fallado {e} con ind={ind}, current_best_fitness={current_best_fitness} y best_fitness={best_fitness}\")\n",
        "\n",
        "      finally:\n",
        "        LEARN.fit(1, lr)\n",
        "      population[ind]=get_params_from_model(LEARN.model)\n",
        "      fit=LEARN.recorder.values[-1][0]\n",
        "      fitness[ind]=fit\n",
        "      temp_current_best_fitness=fit\n",
        "      temp_current_best=population[ind]\n",
        "      evals+=1\n",
        "\n",
        "\n",
        "    improq.append((current_best_fitness-temp_current_best_fitness)/current_best_fitness)\n",
        "\n",
        "    #if temp_current_best_fitness < current_best_fitness:\n",
        "    current_best_fitness = temp_current_best_fitness\n",
        "    current_best = temp_current_best\n",
        "\n",
        "\n",
        "    print(improq)\n",
        "    if np.all(np.array(improq)<0.05):\n",
        "      must_restart=True\n",
        "\n",
        "\n",
        "\n",
        "    #Update the probability to apply LS in next iterations\n",
        "\n",
        "    if current_best_fitness < best_fitness:\n",
        "      best_fitness = np.copy(current_best_fitness)\n",
        "      best_solution = np.copy(current_best)\n",
        "\n",
        "    historic_fitness.append(best_fitness if np.isscalar(best_fitness) else float(best_fitness))\n",
        "    historic_best_solution.append(best_solution)\n",
        "\n",
        "    if must_restart:\n",
        "      random_index = np.random.choice(population_size)\n",
        "      sol = population[random_index]\n",
        "      sol += np.random.normal(0, 0.3, len(sol))\n",
        "\n",
        "      k=0\n",
        "      prevm_cr=0\n",
        "      prevm_f=0\n",
        "      population=[]\n",
        "      for _ in range(population_size-1):\n",
        "        model_copy = copy.deepcopy(model)\n",
        "\n",
        "        # Reset the weights of the model copy\n",
        "        model_copy.apply(init_weights_glorot)\n",
        "\n",
        "        # Add the reinitialized model to the population list\n",
        "        population.append(get_params_from_model(model_copy))\n",
        "      fitness = [err_param(elem, model, dls,task=task) for elem in population]\n",
        "      evals += population_size\n",
        "      population.append(sol)\n",
        "      current_best_fitness=err_param(sol, model, dls,task=task)\n",
        "      current_best=sol\n",
        "      fitness.append(current_best_fitness)\n",
        "      improq=deque([1,1,1], maxlen=3)\n",
        "\n",
        "      print(\"Reiniciado\")\n",
        "      must_restart=False\n",
        "\n",
        "  return best_solution, best_fitness, historic_fitness, historic_best_solution\n"
      ],
      "metadata": {
        "id": "NQb-uvfHqqRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GenAlg\n",
        "\n",
        "# Crossover operator for two childs, both being arrays of parameters\n",
        "#def crossover(parent1, parent2):\n",
        "#  mask = np.random.choice([True, False], size=len(parent1), p=[0.5, 0.5])\n",
        "#  child1 = np.where(mask, parent1, parent2)\n",
        "#  child2 = np.where(mask, parent2, parent1)\n",
        "\n",
        "#  return child1, child2\n",
        "\n",
        "#Operator: choose one entire layer from one of the parents\n",
        "def crossover(parent1, parent2, model):\n",
        "    offset = 0\n",
        "    child1=[]\n",
        "    child2=[]\n",
        "    for param in model.parameters():\n",
        "        param_size = param.numel()\n",
        "        #Generate a random integer 0 or 1\n",
        "        random_int = np.random.randint(0, 2)\n",
        "        if random_int == 0:\n",
        "            child1[offset:offset + param_size] = parent1[offset:offset + param_size]\n",
        "            child2[offset:offset + param_size] = parent2[offset:offset + param_size]\n",
        "        else:\n",
        "            child1[offset:offset + param_size] = parent2[offset:offset + param_size]\n",
        "            child2[offset:offset + param_size] = parent1[offset:offset + param_size]\n",
        "        offset += param_size\n",
        "\n",
        "    return child1, child2\n",
        "\n",
        "#operador bl-alpha\n",
        "\n",
        "def bl_crossover(parent1, parent2, model, alpha=0.2):\n",
        "    child1=[]\n",
        "    child2=[]\n",
        "    offset=0\n",
        "\n",
        "    for param in model.parameters():\n",
        "        param_size = param.numel()\n",
        "        # Compute the distance between parent genes\n",
        "        d = np.abs(parent1[offset:offset + param_size] - parent2[offset:offset + param_size])\n",
        "\n",
        "        # Calculate the lower and upper bounds for the offspring genes\n",
        "        lower_bound = np.minimum(parent1[offset:offset + param_size], parent2[offset:offset + param_size]) - alpha * d\n",
        "        upper_bound = np.maximum(parent1[offset:offset + param_size], parent2[offset:offset + param_size]) + alpha * d\n",
        "\n",
        "        # Generate a random offspring within the specified bounds\n",
        "        child1[offset:offset + param_size] = np.random.uniform(lower_bound, upper_bound)\n",
        "        child2[offset:offset + param_size] = np.random.uniform(lower_bound, upper_bound)\n",
        "\n",
        "        offset += param_size\n",
        "\n",
        "    return child1, child2\n",
        "\n",
        "\n",
        "def mutate(child, mutation_rate=0.01, mutation_std=0.1):\n",
        "\n",
        "    if isinstance(child, list):\n",
        "        child = np.array(child)\n",
        "    # Generate a mask where mutation_rate probability of mutation\n",
        "    mask = np.random.rand(*child.shape) < mutation_rate\n",
        "\n",
        "    # Generate mutation values with normal distribution\n",
        "    mutation = np.random.normal(loc=0.0, scale=mutation_std, size=child.shape)\n",
        "\n",
        "    # Apply mutation: element-wise where mask is True\n",
        "    mutated_child = np.where(mask, child + mutation, child)\n",
        "\n",
        "    return mutated_child\n",
        "\n",
        "\n",
        "\n",
        "def gen_alg(pop_size, population, fit, dls, max_evals, learner):\n",
        "\n",
        "  LEARN = learner\n",
        "\n",
        "  population = population\n",
        "  fitness = fit\n",
        "  evals=pop_size\n",
        "  historic_fitness=[np.min(fitness)]\n",
        "\n",
        "  while evals < max_evals:\n",
        "    # Selection (tournament selection)\n",
        "\n",
        "    #Select 2 groups of 3 individuals\n",
        "    selected_indices_1 = np.random.choice(range(pop_size), size=3, replace=False)\n",
        "    selected_indices_2 = np.random.choice(range(pop_size), size=3, replace=False)\n",
        "\n",
        "\n",
        "    # Get fitness scores for each group\n",
        "    selected_fitness_scores_1 = [fitness[i] for i in selected_indices_1]\n",
        "    selected_fitness_scores_2 = [fitness[i] for i in selected_indices_2]\n",
        "\n",
        "    # Find the best individual in each group\n",
        "    best1_index = selected_indices_1[np.argmin(selected_fitness_scores_1)] # Find the index of the best individual within the selected group\n",
        "    best2_index = selected_indices_2[np.argmin(selected_fitness_scores_2)]\n",
        "    parent1 = population[best1_index]\n",
        "    parent2 = population[best2_index]\n",
        "\n",
        "    # Crossover\n",
        "    child1, child2 = crossover(parent1, parent2, LEARN.model)\n",
        "\n",
        "\n",
        "    # Repeat the process to get another child\n",
        "    selected_indices_1 = np.random.choice(range(pop_size), size=3, replace=False)\n",
        "    selected_indices_2 = np.random.choice(range(pop_size), size=3, replace=False)\n",
        "\n",
        "    selected_fitness_scores_1 = [fitness[i] for i in selected_indices_1]\n",
        "    selected_fitness_scores_2 = [fitness[i] for i in selected_indices_2]\n",
        "\n",
        "    best1_index = selected_indices_1[np.argmin(selected_fitness_scores_1)] # Find the index of the best individual within the selected group\n",
        "    best2_index = selected_indices_2[np.argmin(selected_fitness_scores_2)]\n",
        "\n",
        "    parent1 = population[best1_index]\n",
        "    parent2 = population[best2_index]\n",
        "\n",
        "\n",
        "    # Crossover\n",
        "    child3, child4 = crossover(parent1, parent2, LEARN.model)\n",
        "\n",
        "\n",
        "    #Mutation and getting fitness\n",
        "    children = [child1, child2, child3, child4]\n",
        "    children = [mutate(child) for child in children]\n",
        "    fitness_children=[err_param(child, LEARN.model, dls, task=task) for child in children]\n",
        "    evals+=4\n",
        "\n",
        "\n",
        "    #Getting best two children\n",
        "    best_two_children = np.argsort(fitness_children)[:2]\n",
        "    best_child1 = children[best_two_children[0]]\n",
        "    best_child2 = children[best_two_children[1]]\n",
        "    best_fitness_child1 = fitness_children[best_two_children[0]]\n",
        "    best_fitness_child2 = fitness_children[best_two_children[1]]\n",
        "\n",
        "    #Compare two worst individuals in the population with the best children\n",
        "    worst_indices = np.argsort(fitness)[-2:]\n",
        "    worst_individual1 = population[worst_indices[0]]\n",
        "    worst_individual2 = population[worst_indices[1]]\n",
        "    worst_fitness1 = fitness[worst_indices[0]]\n",
        "    worst_fitness2 = fitness[worst_indices[1]]\n",
        "\n",
        "\n",
        "    #Get the best two candidates and insert them in the population\n",
        "    candidates = [worst_individual1, worst_individual2, best_child1, best_child2]\n",
        "    candidates_fitness = [best_fitness_child1, best_fitness_child2, worst_fitness1, worst_fitness2]\n",
        "\n",
        "    best_two_candidates = np.argsort(candidates_fitness)[:2]\n",
        "    best_candidate1 = candidates[best_two_candidates[0]]\n",
        "    best_candidate2 = candidates[best_two_candidates[1]]\n",
        "\n",
        "    population[worst_indices[0]] = best_candidate1\n",
        "    population[worst_indices[1]] = best_candidate2\n",
        "\n",
        "    historic_fitness.append(np.min(fitness))\n",
        "\n",
        "\n",
        "  return population[np.argmin(fitness)], np.min(fitness), historic_fitness\n",
        "\n"
      ],
      "metadata": {
        "id": "_DC6ipLfqb5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MemAlg\n",
        "\n",
        "def mem_alg(pop_size, population, dls, max_evals, learner):\n",
        "\n",
        "  LEARN= learner\n",
        "\n",
        "  fitness = [err_param(elem, LEARN.model, dls, task=task) for elem in population]\n",
        "  evals=pop_size\n",
        "  historic_fitness=[np.min(fitness)]\n",
        "  generations = (max_evals-evals) // 4\n",
        "  it= generations//10\n",
        "  gen=1\n",
        "\n",
        "  while evals < max_evals and gen<generations:\n",
        "    # Selection (tournament selection)\n",
        "\n",
        "    #Select 2 groups of 3 individuals\n",
        "    selected_indices_1 = np.random.choice(range(pop_size), size=3, replace=False)\n",
        "    selected_indices_2 = np.random.choice(range(pop_size), size=3, replace=False)\n",
        "\n",
        "\n",
        "    # Get fitness scores for each group\n",
        "    selected_fitness_scores_1 = [fitness[i] for i in selected_indices_1]\n",
        "    selected_fitness_scores_2 = [fitness[i] for i in selected_indices_2]\n",
        "\n",
        "    # Find the best individual in each group\n",
        "    best1_index = selected_indices_1[np.argmin(selected_fitness_scores_1)] # Find the index of the best individual within the selected group\n",
        "    best2_index = selected_indices_2[np.argmin(selected_fitness_scores_2)]\n",
        "    parent1 = population[best1_index]\n",
        "    parent2 = population[best2_index]\n",
        "\n",
        "    # Crossover\n",
        "    child1, child2 = crossover(parent1, parent2, LEARN.model)\n",
        "\n",
        "\n",
        "    # Repeat the process to get another child\n",
        "    selected_indices_1 = np.random.choice(range(pop_size), size=3, replace=False)\n",
        "    selected_indices_2 = np.random.choice(range(pop_size), size=3, replace=False)\n",
        "\n",
        "    selected_fitness_scores_1 = [fitness[i] for i in selected_indices_1]\n",
        "    selected_fitness_scores_2 = [fitness[i] for i in selected_indices_2]\n",
        "\n",
        "    best1_index = selected_indices_1[np.argmin(selected_fitness_scores_1)] # Find the index of the best individual within the selected group\n",
        "    best2_index = selected_indices_2[np.argmin(selected_fitness_scores_2)]\n",
        "\n",
        "    parent1 = population[best1_index]\n",
        "    parent2 = population[best2_index]\n",
        "\n",
        "\n",
        "    # Crossover\n",
        "    child3, child4 = crossover(parent1, parent2, LEARN.model)\n",
        "\n",
        "\n",
        "    #Mutation and getting fitness\n",
        "    children = [child1, child2, child3, child4]\n",
        "    children = [mutate(child) for child in children]\n",
        "    fitness_children=[err_param(child, LEARN.model, dls, task=task) for child in children]\n",
        "    evals+=4\n",
        "\n",
        "\n",
        "    #Getting best two children\n",
        "    best_two_children = np.argsort(fitness_children)[:2]\n",
        "    best_child1 = children[best_two_children[0]]\n",
        "    best_child2 = children[best_two_children[1]]\n",
        "    best_fitness_child1 = fitness_children[best_two_children[0]]\n",
        "    best_fitness_child2 = fitness_children[best_two_children[1]]\n",
        "\n",
        "    #Compare two worst individuals in the population with the best children\n",
        "    worst_indices = np.argsort(fitness)[-2:]\n",
        "    worst_individual1 = population[worst_indices[0]]\n",
        "    worst_individual2 = population[worst_indices[1]]\n",
        "    worst_fitness1 = fitness[worst_indices[0]]\n",
        "    worst_fitness2 = fitness[worst_indices[1]]\n",
        "\n",
        "\n",
        "    #Get the best two candidates and insert them in the population\n",
        "    candidates = [worst_individual1, worst_individual2, best_child1, best_child2]\n",
        "    candidates_fitness = [best_fitness_child1, best_fitness_child2, worst_fitness1, worst_fitness2]\n",
        "\n",
        "    best_two_candidates = np.argsort(candidates_fitness)[:2]\n",
        "    best_candidate1 = candidates[best_two_candidates[0]]\n",
        "    best_candidate2 = candidates[best_two_candidates[1]]\n",
        "\n",
        "    population[worst_indices[0]] = best_candidate1\n",
        "    population[worst_indices[1]] = best_candidate2\n",
        "\n",
        "    if gen%it==0:\n",
        "      ind = np.random.randint(0, pop_size)\n",
        "      set_params_to_model(population[ind], LEARN.model)\n",
        "      lr=LEARN.lr_find()\n",
        "      LEARN.fit(1, lr)\n",
        "      population[ind]=get_params_from_model(LEARN.model)\n",
        "      fitness[ind]=LEARN.recorder.values[-1][0]\n",
        "      evals+=1\n",
        "\n",
        "    historic_fitness.append(np.min(fitness))\n",
        "    gen+=1\n",
        "\n",
        "  return population[np.argmin(fitness)], np.min(fitness), historic_fitness\n",
        "\n"
      ],
      "metadata": {
        "id": "0KYw6n1DB0iS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}